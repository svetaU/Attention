{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attnClassifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNKUoPKEbWgiVkpCXQi3TU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetaU/Attention/blob/main/attnClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script implements self-attention model for classification. It borrows some code from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
      ],
      "metadata": {
        "id": "hmpSyH9n364I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "j6smVnr52eft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0-jqDvARfMo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e32b6fb-1bab-44ae-c230-9b60fb5e6f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.seed:Global seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "%tensorflow_version 2.10\n",
        "try:\n",
        "  import einops\n",
        "except ModuleNotFoundError: \n",
        "  !pip install --quiet einops\n",
        "from einops import rearrange\n",
        "from einops import repeat\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: \n",
        "    !pip install --quiet pytorch-lightning>=1.7\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "pl.seed_everything(42)\n",
        "import os\n",
        "CHECKPOINT_PATH = \"../preecl_project/prototype/\"\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting imports"
      ],
      "metadata": {
        "id": "xLaqtGzd2Y6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "#set_matplotlib_formats('svg', 'pdf') # For export\n",
        "try:\n",
        "  import matplotlib_inline\n",
        "except ModuleNotFoundError: \n",
        "  !pip install --quiet matplotlib_inline\n",
        "  import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()"
      ],
      "metadata": {
        "id": "gpXEVQWR2dBp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention module"
      ],
      "metadata": {
        "id": "tHuzglHFfpGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NiptMultiHeadSelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim, heads=2):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.heads=heads\n",
        "        self.dim_head = (int(self.embed_dim / self.heads)) \n",
        "        _dim = self.dim_head * self.heads\n",
        "        self.to_qvk = nn.Linear(self.embed_dim, _dim * 3, bias=False)\n",
        "        self.last_linear = nn.Linear( _dim, self.embed_dim, bias=False)\n",
        "        self.scale_factor = self.dim_head ** -0.5\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.to_qvk.weight)\n",
        "        nn.init.xavier_uniform_(self.last_linear.weight)\n",
        "        \n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        assert x.dim() == 3\n",
        "        qkv = self.to_qvk(x)\n",
        "        q, k, v = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d ', k=3, h=self.heads))\n",
        "        scaled_dot_prod = torch.einsum('b h i d , b h j d -> b h i j', q, k) * self.scale_factor\n",
        "        if mask is not None:\n",
        "            assert mask.shape == scaled_dot_prod.shape[2:]\n",
        "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n",
        "\n",
        "        attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "        values = torch.einsum('b h i j , b h j d -> b h i d', attention, v)\n",
        "        values = rearrange(values, 'b h t d -> b t (h d)')\n",
        "        output = self.last_linear(values)\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output"
      ],
      "metadata": {
        "id": "1YlZ2cQofqnf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "bLi3NpjRf3x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NiptEncoderBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self,embed_dim,num_heads = 2,dim_linear_block=1024, dropout = 0.0):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.num_heads=num_heads\n",
        "        self.dim_linear_block=dim_linear_block\n",
        "        self.dropout=dropout\n",
        "        self.attn_layer = NiptMultiHeadSelfAttention(embed_dim=self.embed_dim, \n",
        "                                                     heads=self.num_heads)\n",
        "        self.norm1 = nn.LayerNorm(self.embed_dim)        \n",
        "        self.norm2 = nn.LayerNorm(self.embed_dim)\n",
        "        self.drop = nn.Dropout(self.dropout)\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, self.dim_linear_block),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(self.dim_linear_block, self.embed_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        y = self.norm1(self.drop(self.attn_layer(x, mask)) + x)\n",
        "        return self.norm2(self.linear_net(y) + y)\n"
      ],
      "metadata": {
        "id": "lPi_NQcFf7S-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NiptAttentionEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim,num_heads,dim_linear_block,dropout,num_layers=6):\n",
        "        super().__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.num_heads=num_heads\n",
        "        self.dim_linear_block=dim_linear_block\n",
        "        self.dropout=dropout\n",
        "        self.num_layers=num_layers\n",
        "        self.layers = nn.ModuleList([NiptEncoderBlock(embed_dim=self.embed_dim,\n",
        "                                    num_heads=self.num_heads,\n",
        "                                    dim_linear_block=self.dim_linear_block, \n",
        "                                    dropout=self.dropout) for _ in range(self.num_layers)])\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        return x\n",
        "    \n",
        "    def get_attention(self,x, mask=None):\n",
        "        attention_maps = []\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.attn_layer(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = l(x, mask=mask)\n",
        "        return attention_maps"
      ],
      "metadata": {
        "id": "LsJaCwxk255n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier module"
      ],
      "metadata": {
        "id": "Hf4Q0SIEh20J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NiptClassifier(pl.LightningModule):\n",
        "    def __init__(self, embed_dim, num_tokens, num_heads, \n",
        "                 num_layers, model_dim,lr, dropout=0.0, input_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self._create_model()\n",
        "        \n",
        "    def _create_model(self):\n",
        "        # Input dim -> Model dim\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Dropout(self.hparams.input_dropout),\n",
        "            nn.Linear(self.hparams.embed_dim, self.hparams.model_dim)\n",
        "        )\n",
        "        # Transformer\n",
        "        self.transformer = NiptAttentionEncoder(\n",
        "                                              embed_dim=self.hparams.model_dim,\n",
        "                                              num_layers=self.hparams.num_layers,\n",
        "                                              dim_linear_block=2*self.hparams.embed_dim,\n",
        "                                              num_heads=self.hparams.num_heads,\n",
        "                                              dropout=self.hparams.dropout)\n",
        "        # Output classifier per sequence elment\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
        "            nn.LayerNorm(self.hparams.model_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(self.hparams.model_dim, 1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.prob = nn.Sequential(\n",
        "            nn.Linear(self.hparams.num_tokens,1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, mask=None, add_positional_encoding=False):\n",
        "        x = self.input_net(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        x = rearrange(x, 'b c h  -> b (c h)')\n",
        "        x = self.prob(x)\n",
        "        return x\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=False):\n",
        "        x = self.input_net(x)\n",
        "        attention_maps = self.transformer.get_attention(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        #scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        #return [optimizer], [{'scheduler': scheduler,'interval': 'step'}]\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "QzGVQoarh79F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdversePredictor(NiptClassifier):\n",
        "\n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        # Fetch data (labels are floats 0. or 1.)\n",
        "        inp_data, labels = batch\n",
        "\n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds = self.forward(inp_data, add_positional_encoding=False)\n",
        "        loss = F.cross_entropy(preds.view(-1), labels.view(-1))\n",
        "        #loss = F.cross_entropy(preds, labels)\n",
        "        acc = ((preds.view(-1) > 0.) == (labels > 0.)).float().mean()\n",
        "\n",
        "        # Logging\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        return loss, acc\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ],
      "metadata": {
        "id": "inFG975J3EDu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auxiliary functions"
      ],
      "metadata": {
        "id": "wxaN6bRs3QOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
        "    if input_data is not None:\n",
        "        input_data = input_data[idx].detach().cpu().numpy()\n",
        "    else:\n",
        "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
        "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
        "\n",
        "    num_heads = attn_maps[0].shape[0]\n",
        "    num_layers = len(attn_maps)\n",
        "    seq_len = input_data.shape[0]\n",
        "    fig_size = 4 if num_heads == 1 else 3\n",
        "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
        "    if num_layers == 1:\n",
        "        ax = [ax]\n",
        "    if num_heads == 1:\n",
        "        ax = [[a] for a in ax]\n",
        "    for row in range(num_layers):\n",
        "        for column in range(num_heads):\n",
        "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
        "            ax[row][column].set_xticks(list(range(seq_len)))\n",
        "            ax[row][column].set_xticklabels(input_data.tolist())\n",
        "            ax[row][column].set_yticks(list(range(seq_len)))\n",
        "            ax[row][column].set_yticklabels(input_data.tolist())\n",
        "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EL42ZGdq3d9y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fakeDataset(data.Dataset):\n",
        "    def __init__(self, num_seqs, num_tokens, token_size, class_fraction):\n",
        "        super().__init__()\n",
        "        self.num_seqs = num_seqs\n",
        "        self.num_tokens = num_tokens\n",
        "        self.token_size = token_size\n",
        "        self.num_elements = int(num_seqs*class_fraction)\n",
        "        self._generate_data()\n",
        "        \n",
        "    def _generate_data(self):\n",
        "        \n",
        "        assert self.num_tokens > 2\n",
        "        self.labels = torch.zeros(self.num_seqs,dtype=torch.float)\n",
        "        mean = repeat(torch.arange(0.,self.num_tokens).unsqueeze(-1).expand(-1,self.token_size), \n",
        "                      't i -> s t i',s=self.num_seqs) #each token has the mean of its index\n",
        "        self.data = torch.normal(mean=mean)\n",
        "        class_inds = torch.randperm(self.num_seqs)[:self.num_elements]\n",
        "        self.labels[class_inds] = 1.\n",
        "        self.data[class_inds,0,:] = torch.empty(self.token_size).normal_(mean=4.)\n",
        "        self.data[class_inds,2,:] = torch.empty(self.token_size).normal_(mean=10.)\n",
        "                \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_seqs\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "cRYCzhQ24TbC",
        "outputId": "8f88bfbb-9c4f-4fd1-89a6-221ad8f84314"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3dd78938902f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mfakeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_seqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test"
      ],
      "metadata": {
        "id": "N5rwSBA-4ZeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = fakeDataset(600,9,64,0.5)\n",
        "train_ds_loader = data.DataLoader(train_ds, batch_size=16, shuffle=True,  drop_last=True)\n",
        "val_ds = fakeDataset(100,9,64,0.5)\n",
        "val_ds_loader = data.DataLoader(val_ds, batch_size=16, shuffle=False,  drop_last=False)\n",
        "test_ds = fakeDataset(300,9,64,0.5)\n",
        "test_ds_loader = data.DataLoader(test_ds, batch_size=16, shuffle=False,  drop_last=False)"
      ],
      "metadata": {
        "id": "-YQl7zOt4dM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nipt(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"testTask\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
        "                         max_epochs=100,\n",
        "                         gradient_clip_val=2)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"testTask.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = AdversePredictor.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = AdversePredictor(**kwargs)\n",
        "        trainer.fit(model, train_ds_loader, val_ds_loader)\n",
        "        model = AdversePredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, train_ds_loader, verbose=False)\n",
        "    val_result = trainer.test(model, val_ds_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_ds_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"], \"train_acc\": train_result[0][\"test_acc\"]}\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model, result"
      ],
      "metadata": {
        "id": "JFGMPXqd3Vbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGDlPdoG4qnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nipt_model, nipt_result = train_nipt(embed_dim=64,\n",
        "                                     model_dim=128,\n",
        "                                     num_tokens = 9,\n",
        "                                     num_heads=4,\n",
        "                                     num_layers=4,\n",
        "                                     dropout=0.1,\n",
        "                                     input_dropout=0.1,\n",
        "                                     lr=5e-4)"
      ],
      "metadata": {
        "id": "7KTxdtgC4g5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train accuracy: {(100.0*nipt_result['train_acc']):4.2f}%\")\n",
        "print(f\"Val accuracy:   {(100.0*nipt_result['val_acc']):4.2f}%\")\n",
        "print(f\"Test accuracy:  {(100.0*nipt_result['test_acc']):4.2f}%\")"
      ],
      "metadata": {
        "id": "c6TjrZHf40Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_data, labels = next(iter(test_ds_loader))\n",
        "inp_data = inp_data.to(device)\n",
        "nipt_model.eval()\n",
        "preds = nipt_model.forward(inp_data, add_positional_encoding=False)\n",
        "attention_maps = nipt_model.get_attention_maps(inp_data, add_positional_encoding=False)"
      ],
      "metadata": {
        "id": "ZJxZH3-n41bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention_maps(input_data=None,attn_maps=attention_maps,idx=14)"
      ],
      "metadata": {
        "id": "dBOW-bEC44p7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}